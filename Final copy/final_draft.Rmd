---
title: "Evolving Ideas and Narratives: Analyzing Raymond Moley's Themes, Keywords, and Sentiment Over Time"
author: "Teona Goderdzishvili"
date: "December 3, 2024 "
output: html_document
---

**<span style="font-size: 16px; font-weight: bold;">Research Questions</span>
**

**1.** How have Moley's ideas and themes evolved over time?

**2.** How has the public perception of Moley, as reflected through descriptive language, changed throughout the years?

I aim to analyze how Moley's most influential ideas developed and transformed across his work. Additionally, I want to understand how he was portrayed in contemporary papers and documents, focusing on the descriptive language and narratives used to characterize him.
My methodological approach uses R for:

- Term frequency analysis to identify key concepts and recurring themes
- Keyword extraction to track specific ideas
- Sentiment analysis to evaluate the emotional tone of the texts
- Temporal trend visualization using ggplot to map changes in sentiment and themes over time

The research combines computational text analysis with historical context to provide insights into both Moley's intellectual evolution and his public reception.

**<span style="font-size: 16px; font-weight: bold;">Preliminary Work on Codebook</span>
**

**Economic Policy:** I'll track the frequency and context of key terms related to Moley's economic policy ideas, such as "new deal" and "reform". This will help me understand the evolution of his policy positions over time. 

**Ideological Stances:** I'll analyze how Moley and his circle's ideological stances, such as "liberalism" and "conservatism", are portrayed in the articles. I'll note if there are any significant changes or shifts in their positions over the years. 

**Character Names and Descriptions:** The articles mention of some of the key names of the state actors. I will gather full names and brief descriptions of their roles and relationships with Moley. 

**Character ID:** Give each character a unique 4-digit number, beginning with 0001. If a character appears in more than one episode, code him or her each time, but use the same ID number.

**Adjectives about Moley:** Moley became known as “the second strongest man in Washington”. They would also refer him as “Columbia university professor”, “Roosevelt’s brain trust”. I will explore how the adjectives related to Moley have been changing over the years. 

Link to the source: https://github.com/wellsdata/CompText_Jour/tree/main/code/Moley_for_students/moley_newsweek/AI_extracted_all 

___

#Appropriate software libraries

```{r}
library(tidyverse)  
library(readtext)
library(kableExtra)
library(tidytext)
library(textdata)
library(pdftools)
library(stringr)  
library(RCurl)
library(ggplot2)

```

#Data

```{r}
#setwd("/Users/teona/Desktop/text analysis/assignment4")
#**rsw comment: this cases the code to fail. it sets the directory to YOUR laptop: /Users/teona/Desktop**

folder_path <- "moley_texts" 
txt_files <- list.files(folder_path, pattern = "\\.txt$", full.names = TRUE)


read_all_texts <- function(folder_path = "moley_texts") {
  if (!dir.exists(folder_path)) {
    stop("Folder '", folder_path, "' not found!")
  }

 
  txt_files <- list.files(
    path = folder_path,
    pattern = "\\.txt$",
    full.names = TRUE
  )
  
  if (length(txt_files) == 0) {
    stop("No .txt files found in folder '", folder_path, "'")
  }

  
  text_data <- readtext(txt_files) %>%
    as_tibble() %>%
    mutate(
      file_number = row_number(),
      doc_id = basename(doc_id),
      doc_id = str_remove(doc_id, "\\.txt$"),
      year = as.numeric(str_extract(doc_id, "\\d{4}"))
    )
  
  return(text_data)
}


text_data <- read_all_texts()

cat("Sample of the data:
")
print(head(text_data))

text_stats <- text_data %>%
  mutate(
    doc_length = nchar(text),
    word_count = str_count(text, "\\w+")
  ) %>%
  summarise(
    total_docs = n(),
    avg_length = mean(doc_length),
    avg_words = mean(word_count),
    min_length = min(doc_length),
    max_length = max(doc_length)
  )
#rsw comment: smart way to print out the stats
cat("
Descriptive Statistics:
")
print(text_stats)


```


#Number of rows and columns of the dataset
```{r}
dim(text_data) 

str(text_data) 

summary(text_data)


nrow(text_data)  
ncol(text_data)  

cat("\nThe dataframe had", nrow(text_data) , "rows. \n" )

```


```{r}

# Set working directory to Final folder (if needed)
# setwd("./Final")  # Uncomment this if you need to set the directory

# First, let's clean and prepare the original data with years
originals_w_dates <- text_data %>%
  mutate(
    year = as.numeric(str_extract(doc_id, "\\d{4}"))
  )

# Read the AI dataset from the Final folder
ai_w_dates <- read.csv("moley_index.csv") %>%
  mutate(
    year = as.numeric(Year)  # Use the Year column from your CSV
  )

# Create yearly counts for the original data
originals_counts <- originals_w_dates %>%
  group_by(year) %>%
  summarise(
    number_of_texts = n()
  )

# Create yearly counts for the AI data
ai_counts <- ai_w_dates %>%
  group_by(Year) %>%
  summarise(
    number_of_texts = n()
  ) %>%
  rename(year = Year)

# Combine both counts and sum up articles per year
yearly_counts <- full_join(originals_counts, ai_counts, by = "year") %>%
  replace_na(list(number_of_texts.x = 0, number_of_texts.y = 0)) %>%
  mutate(
    total_texts = number_of_texts.x + number_of_texts.y
  ) %>%
  select(year, total_texts) %>%
  rename(number_of_texts = total_texts) %>%
  arrange(year)

# Create the visualization
ggplot(yearly_counts, aes(x = factor(year), y = number_of_texts)) +
  geom_bar(stat = "identity", fill = "steelblue", width = 0.7) +
  geom_text(aes(label = number_of_texts), vjust = -0.5) +
  theme_minimal() +
  labs(
    title = "Distribution of Moley Articles by Year",
    subtitle = "Based on Newsweek Articles",
    x = "Year",
    y = "Number of Articles",
    caption = "Source: Moley Newsweek Dataset\nBy: Teona Goderdzishvili"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 20, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank()
  )

# Print the yearly counts
print("Yearly counts of all articles:")
print(yearly_counts)

# Save the plot
ggsave("moley_texts_per_year_combined.png", width = 12, height = 8, dpi = 300)
```

```{r}
# Create bigrams from the text
bigrams <- text_data %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  # Remove stop words
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !is.na(word1),
         !is.na(word2)) %>%
  # Remove numbers
  filter(!str_detect(word1, "^[0-9]+$"),
         !str_detect(word2, "^[0-9]+$")) %>%
  # Remove specific words you want to exclude
  filter(!word1 %in% c("mr", "mrs", "dr", "said", "says", "image", "years", "time", "week", "months", "page", "text", "article", "file", "files", "chapter", "note", "notes", "provided", "roll", "newsweek", "column", "congressional", "afl", "remain"),
         !word2 %in% c("mr", "mrs", "dr", "said", "says", "note", "years", "time", "week", "months", "page", "text", "article", "file", "files", "chapter", "note", "notes", "image", "calls", "september", "october", "layout", "quarterly", "cio", "due")) %>%
  # Remove single-letter words and short words
  filter(str_length(word1) > 1,
         str_length(word2) > 1) %>%
  # Convert to lowercase to better group similar terms
  mutate(
    word1 = tolower(word1),
    word2 = tolower(word2)
  ) %>%
  count(word1, word2, sort = TRUE) %>%
  head(20)

# Your existing bigrams data preparation code stays the same until the plotting part

# Your existing bigrams preparation code stays the same

# Your existing bigrams preparation code stays the same

# Create the plot with even more spacing
ggplot(bigrams, aes(x = reorder(paste(word1, word2), n), y = n)) +
  # Reduce bar width even more for more space
  geom_bar(stat = "identity", fill = "steelblue", width = 0.4) +
  geom_text(aes(label = n), hjust = -0.2, size = 3.5) +
  coord_flip() +
  theme_minimal() +
  labs(
    title = "Top 20 Bigrams in Moley Articles",
    subtitle = "Most Frequent Word Pairs (After Cleaning)",
    x = "Bigram",
    y = "Frequency",
    caption = "Source: Moley Newsweek Dataset\nBy: Teona Goderdzishvili"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    axis.text.y = element_text(size = 10, margin = margin(r = 5)),
    axis.text.x = element_text(size = 9),
    panel.grid.major.x = element_line(color = "gray90"),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_blank(),
    plot.margin = margin(20, 20, 20, 20)
  ) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.15)))

# Save with increased height for even more spacing
ggsave("moley_top_bigrams_cleaned.png", 
       width = 12, 
       height = 40,  # Increased height significantly
       dpi = 300)
```



```{r}
# Libraries and read_all_texts function remain the same...

# Read the data
text_data <- read_all_texts()

sentiment_summary <- text_data %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word,
         !str_detect(word, "^[0-9]+$"),
         str_length(word) > 2) %>%
  filter(!word %in% c("mr", "mrs", "dr", "said", "says", "image", 
                     "page", "text", "article", "file", "newsweek", "column",
                     "responsible", "win", "miracle")) %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(word) %>%
  summarize(
    total_appearances = n(),
    sentiment_value = first(value),
    word_sentiment_total = total_appearances * sentiment_value
  ) %>%
  arrange(desc(abs(word_sentiment_total)))

print("Words sorted by sentiment impact (sentiment_value * appearances):")
print(head(sentiment_summary, 20))

# Create sentiment analysis by article
article_sentiment <- text_data %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word,
         !str_detect(word, "^[0-9]+$"),
         str_length(word) > 2) %>%
  filter(!word %in% c("mr", "mrs", "dr", "said", "says", "image", 
                     "page", "text", "article", "file", "newsweek", "column",
                     "responsible", "win", "miracle")) %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(doc_id, year) %>%
  summarize(
    sentiment_score = sum(value),
    words = n(),
    .groups = "drop"
  ) %>%
  filter(!is.na(year)) %>%
  mutate(normalized_sentiment = sentiment_score/words)

# Create visualization
ggplot(article_sentiment, aes(x = year, y = normalized_sentiment)) +
  geom_point(alpha = 0.6, color = "steelblue", size = 3) +
  geom_smooth(formula = y ~ x, method = "lm", color = "darkred") +
  theme_minimal() +
  labs(
    title = "Sentiment Analysis of Moley Articles",
    x = "Year",
    y = "Sentiment Score (Normalized)",
    caption = "Source: Moley Newsweek Dataset\nBy: Teona Goderdzishvili"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

ggsave("moley_sentiment_analysis.png", width = 12, height = 8, dpi = 300)
```


```{r}

# Load required library
library(topicmodels)

# Create document-term matrix
dtm <- text_data %>%
 unnest_tokens(word, text) %>%
 filter(!word %in% stop_words$word,
        !str_detect(word, "^[0-9]+$"),
        str_length(word) > 2) %>%
 filter(!word %in% c("mr", "mrs", "dr", "said", "says", "image", 
                    "page", "text", "article", "file", "newsweek", "column")) %>%
 count(doc_id, word) %>%
 cast_dtm(doc_id, word, n)

# Fit LDA model with 6 topics
lda_model <- LDA(dtm, k = 6, control = list(seed = 1234))

# Extract top terms for each topic
top_terms <- tidy(lda_model, matrix = "beta") %>%
 group_by(topic) %>%
 slice_max(beta, n = 10) %>%
 arrange(topic, -beta)

# Visualize topics
ggplot(top_terms, aes(reorder(term, beta), beta, fill = factor(topic))) +
 geom_col() +
 facet_wrap(~topic, scales = "free") +
 coord_flip() +
 labs(title = "Top Terms in Each Topic",
      x = "Term",
      y = "Beta Value") +
 theme_minimal()

# Print top terms for each topic
print("Top terms by topic:")
top_terms %>%
 group_by(topic) %>%
 summarize(terms = paste(term, collapse = ", "))
```






```{r}



```

