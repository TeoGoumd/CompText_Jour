---
title: "Evolving Ideas and Narratives: Analyzing Raymond Moley's Themes, Keywords, and Sentiment Over Time"
author: "Teona Goderdzishvili"
date: "December 08, 2024 "
output: html_document
---

**<span style="font-size: 16px; font-weight: bold;">Research Questions</span>
**

**1.** How have Moley's ideas and themes evolved over time?

**2.** How has the public perception of Moley, as reflected through descriptive language, changed throughout the years?

I aim to analyze how Moley's most influential ideas developed and transformed across his work. Additionally, I want to understand how he was portrayed in contemporary papers and documents, focusing on the descriptive language and narratives used to characterize him.
My methodological approach uses R for:

- Term frequency analysis to identify key concepts and recurring themes
- Keyword extraction to track specific ideas
- Sentiment analysis to evaluate the emotional tone of the texts
- Temporal trend visualization using ggplot to map changes in sentiment and themes over time

The research combines computational text analysis with historical context to provide insights into both Moley's intellectual evolution and his public reception.

**<span style="font-size: 16px; font-weight: bold;">Preliminary Work on Codebook</span>
**

**Economic Policy:** I'll track the frequency and context of key terms related to Moley's economic policy ideas, such as "new deal" and "reform". This will help me understand the evolution of his policy positions over time. 

**Ideological Stances:** I'll analyze how Moley and his circle's ideological stances, such as "liberalism" and "conservatism", are portrayed in the articles. I'll note if there are any significant changes or shifts in their positions over the years. 

**Character Names and Descriptions:** The articles mention of some of the key names of the state actors. I will gather full names and brief descriptions of their roles and relationships with Moley. 

**Character ID:** Give each character a unique 4-digit number, beginning with 0001. If a character appears in more than one episode, code him or her each time, but use the same ID number.

**Adjectives about Moley:** Moley became known as “the second strongest man in Washington”. They would also refer him as “Columbia university professor”, “Roosevelt’s brain trust”. I will explore how the adjectives related to Moley have been changing over the years. 

Link to the source: https://github.com/wellsdata/CompText_Jour/tree/main/code/Moley_for_students/moley_newsweek/AI_extracted_all 

___

**<span style="font-size: 16px; font-weight: bold;">Appropriate software libraries</span>
**

```{r}
library(tidyverse)  
library(readtext)
library(kableExtra)
library(tidytext)
library(textdata)
library(pdftools)
library(stringr)  
library(RCurl)
library(ggplot2)
library(topicmodels)
```

```{r}
text_data <- read.csv("./moley_texts/moley_extracted_index.csv") %>% 
  mutate(
    new_path = str_replace_all(new_path, "AI_extracted_all", "moley_texts"),
    date = ymd(date)) 
```


**<span style="font-size: 16px; font-weight: bold;">Number of rows and columns of the dataset</span>
**
```{r}
dim(text_data) 

str(text_data) 

summary(text_data)


nrow(text_data)  
ncol(text_data)  

cat("\nThe dataframe had", nrow(text_data) , "rows. \n" )

```

```{r}
# Combine both counts and sum up articles per year
yearly_counts <- text_data %>% 
  count(Year)

# Create the visualization
ggplot(yearly_counts, aes(x = factor(Year), y = n)) +
  geom_bar(stat = "identity", fill = "steelblue", width = 0.7) +
  geom_text(aes(label = n), vjust = -0.5) +
  scale_y_continuous(limits=c(0,20)) +
  theme_minimal() +
  labs(
    title = "Distribution of Moley Articles by Year",
    subtitle = "Based on Newsweek Articles",
    x = "Year",
    y = "Number of Articles",
    caption = "Source: Moley Newsweek Dataset\nBy: Teona Goderdzishvili"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 20, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank()
  )

# Print the yearly counts
print("Yearly counts of all articles:")
print(yearly_counts)

# Save the plot
ggsave("moley_texts_per_year_combined.png", width = 12, height = 8, dpi = 300)
```

```{r}
folder_path <- "moley_texts" 
txt_files <- list.files(folder_path, pattern = "\\.txt$", full.names = TRUE)

read_all_texts <- function(folder_path = "moley_texts") {
  if (!dir.exists(folder_path)) {
    stop("Folder '", folder_path, "' not found!")
  }

  txt_files <- list.files(
    path = folder_path,
    pattern = "\\.txt$",
    full.names = TRUE
  )
  
  if (length(txt_files) == 0) {
    stop("No .txt files found in folder '", folder_path, "'")
  }

  text_files <- readtext(txt_files) %>%
    as_tibble() %>%
    mutate(
      file_number = row_number(),
      doc_id = basename(doc_id),
      doc_id = str_remove(doc_id, "\\.txt$"),
      year = as.numeric(str_extract(doc_id, "\\d{4}"))
    )
  
  return(text_files)
}


text_files <- read_all_texts()

cat("Sample of the data:
")
```


```{r}
# Create bigrams from the text
bigrams <- text_files %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  # Remove stop words
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !is.na(word1),
         !is.na(word2)) %>%

  filter(!str_detect(word1, "^[0-9]+$"),
         !str_detect(word2, "^[0-9]+$")) %>%
  # Remove specific words you want to exclude
  filter(!word1 %in% c("mr", "mrs", "dr", "said", "says", "image", "years", "time", "week", "months", "page", "text", "article", "file", "files", "chapter", "note", "notes", "provided", "roll", "newsweek", "column", "congressional", "afl", "remain", "columns", "overlapping", "pdf", "avoid", "minor", "common", "presi"),
         !word2 %in% c("mr", "mrs", "dr", "said", "says", "note", "years", "time", "week", "months", "page", "text", "article", "file", "files", "chapter", "note", "notes", "image", "calls", "september", "october", "layout", "quarterly", "cio", "due", "perspective", "columns", "formatted", "overlapping", "inoccurances", "sense", "dent")) %>%
  # Remove single-letter words and short words
  filter(str_length(word1) > 1,
         str_length(word2) > 1) %>%
  # Convert to lowercase to better group similar terms
  mutate(
    word1 = tolower(word1),
    word2 = tolower(word2)
  ) %>%
  count(word1, word2, sort = TRUE) %>%
  head(20)

# Create the plot with even more spacing
ggplot(bigrams, aes(x = reorder(paste(word1, word2), n), y = n)) +
  # Reduce bar width even more for more space
  geom_bar(stat = "identity", fill = "steelblue", width = 0.4) +
  geom_text(aes(label = n), hjust = -0.2, size = 3.5) +
  coord_flip() +
  theme_minimal() +
  labs(
    title = "Top 20 Bigrams in Moley Articles",
    subtitle = "Most Frequent Word Pairs (After Cleaning)",
    x = "Bigram",
    y = "Frequency",
    caption = "Source: Moley Newsweek Dataset\nBy: Teona Goderdzishvili"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    axis.text.y = element_text(size = 10, margin = margin(r = 5)),
    axis.text.x = element_text(size = 9),
    panel.grid.major.x = element_line(color = "gray90"),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_blank(),
    plot.margin = margin(20, 20, 20, 20)
  ) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.15)))

# Save with increased height for even more spacing
ggsave("moley_top_bigrams_cleaned.png", 
       width = 12, 
       height = 40,  # Increased height significantly
       dpi = 300)
```


```{r}
text_data <- read_all_texts()

sentiment_summary <- text_data %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word,
         !str_detect(word, "^[0-9]+$"),
         str_length(word) > 2) %>%
  filter(!word %in% c("mr", "mrs", "dr", "said", "says", "image", 
                     "page", "text", "article", "file", "newsweek", "column",
                     "responsible", "win", "miracle", "grand", "true", "lost", "support","benefits")) %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(word) %>%
  summarize(
    total_appearances = n(),
    sentiment_value = first(value),
    word_sentiment_total = total_appearances * sentiment_value
  ) %>%
  arrange(desc(abs(word_sentiment_total)))

print("Words sorted by sentiment impact (sentiment_value * appearances):")
print(head(sentiment_summary, 20))

# Create sentiment analysis by article
article_sentiment <- text_data %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word,
         !str_detect(word, "^[0-9]+$"),
         str_length(word) > 2) %>%
  filter(!word %in% c("mr", "mrs", "dr", "said", "says", "image", 
                     "page", "text", "article", "file", "newsweek", "column",
                     "responsible", "win", "miracle", "grand", "true", "lost")) %>%
  inner_join(get_sentiments("afinn"), by = "word", "support") %>%
  group_by(doc_id, year) %>%
  summarize(
    sentiment_score = sum(value),
    words = n(),
    .groups = "drop"
  ) %>%
  filter(!is.na(year)) %>%
  mutate(normalized_sentiment = sentiment_score/words)

# Create visualization
ggplot(article_sentiment, aes(x = year, y = normalized_sentiment)) +
  geom_point(alpha = 0.6, color = "steelblue", size = 3) +
  geom_smooth(formula = y ~ x, method = "lm", color = "darkred") +
  theme_minimal() +
  labs(
    title = "Sentiment Analysis of Moley Articles",
    subtitle = "Based on articles published in 1942-1960",
    x = "Year",
    y = "Sentiment Score (Normalized)",
    caption = "Source: Moley Newsweek Dataset\nBy: Teona Goderdzishvili"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 12),  # Added this line to center subtitle
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

ggsave("moley_sentiment_analysis.png", width = 12, height = 8, dpi = 300)
```


**<span style="font-size: 16px; font-weight: bold;">Topic modeling</span>
**
```{r}

# 1. Create Document-Term Matrix (DTM) and LDA model
dtm <- text_data %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word,
         !str_detect(word, "^[0-9]+$"),
         str_length(word) > 2) %>%
  filter(!word %in% c("mr", "mrs", "dr", "said", "says", "image", 
                     "page", "text", "article", "file", "newsweek", "column")) %>%
  count(doc_id, word) %>%
  cast_dtm(doc_id, word, n)

lda_model <- LDA(dtm, k = 6, control = list(seed = 1234))

# Define custom topic labels
topic_labels <- c(
  "1" = "Government & Politics",
  "2" = "Economic Policy",
  "3" = "Social Reform",
  "4" = "International Affairs",
  "5" = "Public Administration",
  "6" = "Labor Relations"
)

# Extract top terms from topics
top_terms <- tidy(lda_model, matrix = "beta") %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  arrange(topic, -beta) %>%
  mutate(topic_label = recode(as.character(topic), !!!topic_labels))  # Add topic labels

# Visualization with "Value" label and hidden numerical y-axis values
ggplot(top_terms, aes(reorder(term, beta), beta, fill = topic_label)) +
  geom_col(width = 0.5) +
  facet_wrap(~topic, scales = "free") +
  coord_flip() +
  labs(
    title = "Top Terms in Each Topic",
    subtitle = "Based on LDA Topic Modeling Analysis",
    x = "Term",
    y = "Value",  # Retain "Value" on the y-axis
    fill = "Topics",  # Legend for topics
    caption = "Source: Moley Newsweek Dataset\nBy: Teona Goderdzishvili"
  ) +
  scale_y_continuous(labels = NULL) +  # Remove numerical labels on y-axis
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    axis.text.y = element_text(size = 10, margin = margin(r = 5, l = 5)),
    strip.text = element_text(size = 12, face = "bold"),  # Keep original column titles
    legend.position = "right",  # Add legend to the right
    legend.title = element_text(size = 12, face = "bold"),
    legend.text = element_text(size = 10),
    plot.margin = margin(t = 20, r = 20, b = 20, l = 20)  # Add margin around the plot
  )

# Save the updated plot
ggsave(
  "moley_topics_analysis_value_no_numbers.png", 
  width = 12, 
  height = 15, 
  dpi = 300
)


```

___

**<span style="font-size: 16px; font-weight: bold;">Analyzing Raymond Moley's Newsweek Articles (1942-1960): A Computational Text Analysis</span>
**


By using computational analysis of Raymond Moley's Newsweek articles from 1942 to 1960, I tried to reveal specific patterns in his journalistic output and thematic focus. The distribution analysis shows varying levels of publication activity across the years, as documented in the yearly counts data that I had. I quantified this variation through the publication frequency data available in the dataset I had, which captures the complete timeline of his Newsweek contributions during this period.

In total, I had 110 documents published from 1942 to 1960. Looking at the yearly distribution of articles, I can see that publication activity peaked in the late 1950s, with 15 articles in 1959 and 12 articles in 1958. This increase in coverage during the late 1950s represents the highest concentration of articles about Moley in my dataset. The data comes from the 'moley_extracted_index.csv' file, which contains 110 rows (representing individual articles) and 12 columns of metadata and content. This dataset provides a comprehensive view of Moley's Newsweek contributions over an 18-year period, allowing to track changes in both the frequency of his articles and their content.

Using R for the analysis, I loaded several key libraries including tidyverse, readtext, tidytext, and topicmodels to process and analyze the text data. After running initial analyses, I found I had 110 rows and 12 columns in total, giving me a substantial dataset to work with. The year-by-year article count analysis reveals varying levels of publishing activity. This distribution helps to understand the intensity of Moley's engagement with public discourse during different periods of his career at Newsweek. As mentioned at the beginning, the most articles (15) about Moley were published in 1959, and 12 articles were published the year before in 1958. This might indicate the key changes in his career as well as the ongoings in the country. 

The bigram analysis, after careful cleaning and preprocessing, revealed frequent word pairs in Moley's articles. By removing stop words and common terms, I was able to identify the most significant conceptual relationships in his writing. The top three most frequent bigrams were "Raymond Moley," "Republican party," and "federal government," indicating his focus on political institutions and governance. This analysis helps understand how concepts were connected in his writing and which ideas were frequently discussed together.

Through Latent Dirichlet Allocation (LDA) topic modeling with six distinct topics, I identified key thematic clusters in Moley's writing. Each topic showed specific patterns of word usage that help characterize his areas of focus:

**Topic 1 (Government & Politics)**: Shows high beta values for terms related to governmental processes and political discourse. The model identifies this as a dominant theme in Moley's writing across the period studied. Key terms include "republican party," "government," "national," and "war," showing his focus on major political institutions and events.

**Topic 2 (Economic Policy)**: Emerges as a significant theme with terms focused on economic concepts and policy matters. Terms like "congress," "government," "tax," and "business" appear prominently, reflecting Moley's sustained engagement with economic issues and their intersection with governmental policy.

**Topic 3 (Social Reform)**: Captures discussions of social policy and reform initiatives. The prominence of terms like "people," "government," "party," and "president" suggests a focus on how political leadership addressed social issues.

**Topic 4 (International Affairs)**: This topic reveals Moley's engagement with global issues, featuring terms like "political," "labor," "republican," and "people," showing how he connected domestic and international concerns.

**Topic 5 (Public Administration)**: Demonstrates Moley's interest in governance mechanics with terms like "federal," "school," "law," and "government," indicating his attention to institutional frameworks and public policy implementation.

**Topic 6 (Labor Relations)**: Highlights labor-related discussions with terms like "president," "senate," "republican," and "congress," showing how Moley approached labor issues within the broader political context.

The sentiment analysis using the AFINN lexicon provided particular insights into the emotional tone of Moley's articles. By examining word_sentiment_total (calculated by multiplying total appearances by sentiment value), I found interesting patterns. The term "war" appeared most frequently with 72 occurrences, while terms like "strong" and "united" were the next most frequent positive descriptors. This suggests a complex emotional landscape in his writing, balancing discussion of conflict with emphasis on unity and strength.

These computational analyses collectively reveal patterns in Moley's writing that might not be immediately apparent through traditional reading. The combination of topic modeling, bigram analysis, and sentiment analysis provides a multi-faceted view of his journalistic work, showing both what he wrote about and how he approached these subjects.

The findings suggest that Moley maintained consistent interest in certain core themes - particularly governance and economic policy - while the emotional tenor of his writing varied depending on the subject matter and timing. This computational approach helps quantify and validate observations about his writing style and thematic focus across nearly two decades of journalistic work.